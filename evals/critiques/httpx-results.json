{
  "target": "encode/httpx",
  "url": "https://github.com/encode/httpx",
  "date": "2026-02-14T18:24:26.162255",
  "gremlin_version": "0.2.0",
  "threshold": 70,
  "depth": "deep",
  "areas": [
    {
      "area": "Authentication System",
      "files": [
        "httpx/_auth.py"
      ],
      "result": {
        "scope": "HTTP authentication system - BasicAuth, DigestAuth, NetRCAuth with pluggable auth flows, challenge-response patterns, and credential handling",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if concurrent requests increment `_nonce_count` simultaneously, causing duplicate or out-of-order nonce counts?",
            "impact": "Server rejects requests with duplicate `nc` values, causing authentication failures under load. Multiple threads/async tasks accessing the same `DigestAuth` instance create race conditions.",
            "domains": [
              "Concurrency + Auth"
            ],
            "title": "Digest Auth Nonce Count Race Condition"
          },
          {
            "severity": "CRITICAL",
            "confidence": 90,
            "scenario": "What if an exception occurs inside the `auth_flow` generator while credentials are in local variables, exposing them in stack traces?",
            "impact": "Username/password appear in logs, error monitoring systems, or debug output when `_build_auth_header` or parsing fails mid-flow.",
            "domains": [
              "Auth + Error Paths"
            ],
            "title": "Credential Leakage via Generator Exception Stack Traces"
          },
          {
            "severity": "HIGH",
            "confidence": 80,
            "scenario": "What if the `.netrc` file contains malformed entries or is corrupted, causing the `netrc.netrc()` constructor to fail during request processing?",
            "impact": "Authentication completely broken for all requests using NetRCAuth, no graceful fallback. The lazy import means failures happen at request time, not initialization.",
            "domains": [
              "External Dependencies + Error Paths"
            ],
            "title": "NetRC File Parsing with Malformed Entries"
          },
          {
            "severity": "HIGH",
            "confidence": 85,
            "scenario": "What if an attacker captures a 401 response with digest challenge and replays it to trigger credential disclosure in subsequent requests?",
            "impact": "`_last_challenge` is cached and reused. If an attacker can inject stale/malicious challenges, they might be able to influence the auth header generation process.",
            "domains": [
              "Auth + State & Data"
            ],
            "title": "Digest Challenge Replay Attack Window"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if a malicious server sends extremely large or deeply nested `WWW-Authenticate` headers causing `parse_http_list()` to consume excessive memory?",
            "impact": "DoS through memory exhaustion. The parsing happens before validation, and there's no apparent size limit on header processing.",
            "domains": [
              "Resource Limits + Input Validation"
            ],
            "title": "Memory Exhaustion from Malformed WWW-Authenticate Headers"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if username/password contain binary data or are extremely long, causing the base64-encoded auth header to exceed HTTP header size limits?",
            "impact": "Requests rejected by proxies/servers with header size limits. The code doesn't validate credential length before encoding.",
            "domains": [
              "Input Validation + Resource Limits"
            ],
            "title": "Base64 Encoding Bloat with Large Credentials"
          },
          {
            "severity": "MEDIUM",
            "confidence": 85,
            "scenario": "What if the server sends `algorithm=\"md5\"` (lowercase) but the lookup dictionary uses uppercase keys, causing a KeyError?",
            "impact": "Authentication fails due to case mismatch. Code uses `challenge.algorithm.upper()` for lookup but stores the original case in the challenge object.",
            "domains": [
              "Input Validation + Auth"
            ],
            "title": "Algorithm Case Sensitivity Mismatch"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if the server always returns 401 even with correct digest auth, causing the flow to infinitely retry authentication?",
            "impact": "Infinite loop consuming resources. The generator doesn't track retry counts or detect authentication failure loops.",
            "domains": [
              "Error Paths + Resource Limits"
            ],
            "title": "Infinite Auth Loop on Persistent 401s"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if the server's nonce expires but doesn't provide a `stale=true` parameter, causing the client to retry with the same stale challenge indefinitely?",
            "impact": "Authentication appears to work initially but fails on subsequent requests using cached challenge data. No mechanism to detect or handle stale nonces.",
            "domains": [
              "Auth + State & Data"
            ],
            "title": "Stale Nonce Handling Without Server Guidance"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if the response contains malicious cookies that get automatically attached to the retry request via `Cookies(response.cookies).set_cookie_header(request=request)`?",
            "impact": "Unvalidated cookies from auth responses are blindly forwarded to retry requests, potentially enabling session fixation or injection attacks.",
            "domains": [
              "Auth + Input Validation"
            ],
            "title": "Cookie Injection During Auth Retry"
          }
        ],
        "matched_domains": [
          "auth"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 10,
          "critical": 2,
          "high": 3,
          "medium": 5,
          "low": 0
        }
      },
      "raw_response": "## Risk Analysis: HTTP Authentication System\n\n### \ud83d\udd34 CRITICAL (85%)\n\n**Digest Auth Nonce Count Race Condition**\n\n> What if concurrent requests increment `_nonce_count` simultaneously, causing duplicate or out-of-order nonce counts?\n\n- **Impact:** Server rejects requests with duplicate `nc` values, causing authentication failures under load. Multiple threads/async tasks accessing the same `DigestAuth` instance create race conditions.\n- **Domain:** Concurrency + Auth\n\n### \ud83d\udd34 CRITICAL (90%)\n\n**Credential Leakage via Generator Exception Stack Traces**\n\n> What if an exception occurs inside the `auth_flow` generator while credentials are in local variables, exposing them in stack traces?\n\n- **Impact:** Username/password appear in logs, error monitoring systems, or debug output when `_build_auth_header` or parsing fails mid-flow.\n- **Domain:** Auth + Error Paths\n\n### \ud83d\udfe0 HIGH (80%)\n\n**NetRC File Parsing with Malformed Entries**\n\n> What if the `.netrc` file contains malformed entries or is corrupted, causing the `netrc.netrc()` constructor to fail during request processing?\n\n- **Impact:** Authentication completely broken for all requests using NetRCAuth, no graceful fallback. The lazy import means failures happen at request time, not initialization.\n- **Domain:** External Dependencies + Error Paths\n\n### \ud83d\udfe0 HIGH (85%)\n\n**Digest Challenge Replay Attack Window**\n\n> What if an attacker captures a 401 response with digest challenge and replays it to trigger credential disclosure in subsequent requests?\n\n- **Impact:** `_last_challenge` is cached and reused. If an attacker can inject stale/malicious challenges, they might be able to influence the auth header generation process.\n- **Domain:** Auth + State & Data\n\n### \ud83d\udfe0 HIGH (75%)\n\n**Memory Exhaustion from Malformed WWW-Authenticate Headers**\n\n> What if a malicious server sends extremely large or deeply nested `WWW-Authenticate` headers causing `parse_http_list()` to consume excessive memory?\n\n- **Impact:** DoS through memory exhaustion. The parsing happens before validation, and there's no apparent size limit on header processing.\n- **Domain:** Resource Limits + Input Validation\n\n### \ud83d\udfe1 MEDIUM (80%)\n\n**Base64 Encoding Bloat with Large Credentials**\n\n> What if username/password contain binary data or are extremely long, causing the base64-encoded auth header to exceed HTTP header size limits?\n\n- **Impact:** Requests rejected by proxies/servers with header size limits. The code doesn't validate credential length before encoding.\n- **Domain:** Input Validation + Resource Limits\n\n### \ud83d\udfe1 MEDIUM (85%)\n\n**Algorithm Case Sensitivity Mismatch**\n\n> What if the server sends `algorithm=\"md5\"` (lowercase) but the lookup dictionary uses uppercase keys, causing a KeyError?\n\n- **Impact:** Authentication fails due to case mismatch. Code uses `challenge.algorithm.upper()` for lookup but stores the original case in the challenge object.\n- **Domain:** Input Validation + Auth\n\n### \ud83d\udfe1 MEDIUM (75%)\n\n**Infinite Auth Loop on Persistent 401s**\n\n> What if the server always returns 401 even with correct digest auth, causing the flow to infinitely retry authentication?\n\n- **Impact:** Infinite loop consuming resources. The generator doesn't track retry counts or detect authentication failure loops.\n- **Domain:** Error Paths + Resource Limits\n\n### \ud83d\udfe1 MEDIUM (80%)\n\n**Stale Nonce Handling Without Server Guidance**\n\n> What if the server's nonce expires but doesn't provide a `stale=true` parameter, causing the client to retry with the same stale challenge indefinitely?\n\n- **Impact:** Authentication appears to work initially but fails on subsequent requests using cached challenge data. No mechanism to detect or handle stale nonces.\n- **Domain:** Auth + State & Data\n\n### \ud83d\udfe1 MEDIUM (75%)\n\n**Cookie Injection During Auth Retry**\n\n> What if the response contains malicious cookies that get automatically attached to the retry request via `Cookies(response.cookies).set_cookie_header(request=request)`?\n\n- **Impact:** Unvalidated cookies from auth responses are blindly forwarded to retry requests, potentially enabling session fixation or injection attacks.\n- **Domain:** Auth + Input Validation"
    },
    {
      "area": "Connection Pooling & Transport",
      "files": [
        "httpx/_transports/default.py"
      ],
      "result": {
        "scope": "HTTP connection pooling and transport layer - HTTPTransport, AsyncHTTPTransport, HTTP/2 multiplexing, connection reuse, socket options, retry logic",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if all connections are held by long-running requests when `max_connections` is reached?",
            "impact": "New requests hang indefinitely waiting for available connections, creating cascading timeout failures across the application. Users see timeouts, background jobs fail, health checks fail.",
            "domains": [
              "Resource Limits + Concurrency"
            ],
            "title": "Connection Pool Starvation Cascade"
          },
          {
            "severity": "CRITICAL",
            "confidence": 80,
            "scenario": "What if HTTP/2 multiplexing hits the server's max concurrent streams limit but httpx doesn't handle the GOAWAY gracefully?",
            "impact": "Requests fail with obscure protocol errors instead of falling back to new connections. Particularly dangerous because HTTP/2 defaults to `http2=False`, so teams enabling it may not test this edge case thoroughly.",
            "domains": [
              "External Dependencies + Resource Limits"
            ],
            "title": "HTTP/2 Stream Limit Exhaustion"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if a keepalive connection gets closed by the server between the pool's availability check and actual request sending?",
            "impact": "Request fails with connection errors instead of transparently retrying on a fresh connection. Creates intermittent failures that are hard to reproduce in testing.",
            "domains": [
              "Concurrency + State & Data"
            ],
            "title": "Keepalive Connection Reuse Race"
          },
          {
            "severity": "HIGH",
            "confidence": 85,
            "scenario": "What if proxy authentication headers leak into the actual request headers sent to the target server?",
            "impact": "Sensitive proxy credentials exposed to downstream servers, potential security breach if servers log full request headers.",
            "domains": [
              "Configuration + External Dependencies"
            ],
            "title": "Proxy Authentication Header Persistence"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if custom socket options conflict with HTTP/2 connection requirements or get applied after connection negotiation?",
            "impact": "Connections fail to establish properly or negotiate wrong protocols. Silent degradation where connections work but with poor performance characteristics.",
            "domains": [
              "Configuration + External Dependencies"
            ],
            "title": "Socket Option Application Timing"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if the Unix Domain Socket path gets deleted/recreated between connection attempts during server restart?",
            "impact": "Connection attempts fail with \"No such file or directory\" errors instead of retrying. Particularly problematic during rolling deployments.",
            "domains": [
              "External Dependencies + State & Data"
            ],
            "title": "UDS Path Resolution Race"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if the `retries=1` config causes non-idempotent requests (POST/PUT with side effects) to execute twice when connection fails mid-request?",
            "impact": "Duplicate orders, payments, or other side effects. The httpcore layer may retry connection failures without knowing if the request body was partially sent.",
            "domains": [
              "Error Paths + Concurrency"
            ],
            "title": "Retry Logic Double-Execution"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if `verify=True` with custom `cert` parameters creates an SSL context that validates the client cert but ignores server cert validation?",
            "impact": "Man-in-the-middle attacks possible despite appearing to have SSL verification enabled. Subtle misconfiguration that passes basic testing.",
            "domains": [
              "Configuration + External Dependencies"
            ],
            "title": "SSL Context Validation Bypass"
          },
          {
            "severity": "MEDIUM",
            "confidence": 70,
            "scenario": "What if the transport is used after `__exit__` is called but before garbage collection, or if `__exit__` raises an exception leaving the pool in invalid state?",
            "impact": "Connections remain open consuming resources, or subsequent requests fail with obscure state errors. Memory leaks in long-running processes.",
            "domains": [
              "Resource Limits + Error Paths"
            ],
            "title": "Connection Pool Context Manager Leak"
          }
        ],
        "matched_domains": [
          "database"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 9,
          "critical": 2,
          "high": 3,
          "medium": 4,
          "low": 0
        }
      },
      "raw_response": "## \ud83d\udd34 CRITICAL (85%)\n\n**Connection Pool Starvation Cascade**\n\n> What if all connections are held by long-running requests when `max_connections` is reached?\n\n- **Impact:** New requests hang indefinitely waiting for available connections, creating cascading timeout failures across the application. Users see timeouts, background jobs fail, health checks fail.\n- **Domain:** Resource Limits + Concurrency\n\n## \ud83d\udd34 CRITICAL (80%)\n\n**HTTP/2 Stream Limit Exhaustion**\n\n> What if HTTP/2 multiplexing hits the server's max concurrent streams limit but httpx doesn't handle the GOAWAY gracefully?\n\n- **Impact:** Requests fail with obscure protocol errors instead of falling back to new connections. Particularly dangerous because HTTP/2 defaults to `http2=False`, so teams enabling it may not test this edge case thoroughly.\n- **Domain:** External Dependencies + Resource Limits\n\n## \ud83d\udfe0 HIGH (90%)\n\n**Keepalive Connection Reuse Race**\n\n> What if a keepalive connection gets closed by the server between the pool's availability check and actual request sending?\n\n- **Impact:** Request fails with connection errors instead of transparently retrying on a fresh connection. Creates intermittent failures that are hard to reproduce in testing.\n- **Domain:** Concurrency + State & Data\n\n## \ud83d\udfe0 HIGH (85%)\n\n**Proxy Authentication Header Persistence**\n\n> What if proxy authentication headers leak into the actual request headers sent to the target server?\n\n- **Impact:** Sensitive proxy credentials exposed to downstream servers, potential security breach if servers log full request headers.\n- **Domain:** Configuration + External Dependencies\n\n## \ud83d\udfe0 HIGH (75%)\n\n**Socket Option Application Timing**\n\n> What if custom socket options conflict with HTTP/2 connection requirements or get applied after connection negotiation?\n\n- **Impact:** Connections fail to establish properly or negotiate wrong protocols. Silent degradation where connections work but with poor performance characteristics.\n- **Domain:** Configuration + External Dependencies\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**UDS Path Resolution Race**\n\n> What if the Unix Domain Socket path gets deleted/recreated between connection attempts during server restart?\n\n- **Impact:** Connection attempts fail with \"No such file or directory\" errors instead of retrying. Particularly problematic during rolling deployments.\n- **Domain:** External Dependencies + State & Data\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**Retry Logic Double-Execution**\n\n> What if the `retries=1` config causes non-idempotent requests (POST/PUT with side effects) to execute twice when connection fails mid-request?\n\n- **Impact:** Duplicate orders, payments, or other side effects. The httpcore layer may retry connection failures without knowing if the request body was partially sent.\n- **Domain:** Error Paths + Concurrency\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**SSL Context Validation Bypass**\n\n> What if `verify=True` with custom `cert` parameters creates an SSL context that validates the client cert but ignores server cert validation?\n\n- **Impact:** Man-in-the-middle attacks possible despite appearing to have SSL verification enabled. Subtle misconfiguration that passes basic testing.\n- **Domain:** Configuration + External Dependencies\n\n## \ud83d\udfe1 MEDIUM (70%)\n\n**Connection Pool Context Manager Leak**\n\n> What if the transport is used after `__exit__` is called but before garbage collection, or if `__exit__` raises an exception leaving the pool in invalid state?\n\n- **Impact:** Connections remain open consuming resources, or subsequent requests fail with obscure state errors. Memory leaks in long-running processes.\n- **Domain:** Resource Limits + Error Paths"
    },
    {
      "area": "Redirect Handling",
      "files": [
        "httpx/_client.py"
      ],
      "result": {
        "scope": "HTTP redirect handling - method transformation on 301/302/303, header filtering across origins, auth header stripping, max_redirects, circular redirect detection",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if an attacker controls a redirect destination and the auth header isn't properly stripped when crossing origins?",
            "impact": "API keys, bearer tokens, or credentials leaked to attacker-controlled domains. Complete account compromise.",
            "domains": [
              "auth",
              "api"
            ],
            "title": "Auth Header Persistence Across Malicious Redirects"
          },
          {
            "severity": "CRITICAL",
            "confidence": 90,
            "scenario": "What if circular redirect detection fails when URLs differ by query params, fragments, or encoding but resolve to same resource?",
            "impact": "Memory exhaustion from growing redirect chain tracking, CPU spinning until timeout, DoS affecting all client requests.",
            "domains": [
              "universal (Resource Limits)"
            ],
            "title": "Infinite Redirect Loop Resource Exhaustion"
          },
          {
            "severity": "HIGH",
            "confidence": 80,
            "scenario": "What if a POST to a payment endpoint redirects cross-origin and gets downgraded to GET, exposing sensitive data in URL logs?",
            "impact": "Payment details, personal data exposed in server logs, referrer headers, browser history. GDPR/compliance violations.",
            "domains": [
              "auth",
              "api"
            ],
            "title": "Method Downgrade on Cross-Origin POST Redirect"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if the same client follows redirects concurrently and redirect count tracking gets corrupted between threads?",
            "impact": "max_redirects bypass leading to infinite loops, or premature termination of legitimate requests.",
            "domains": [
              "universal (Concurrency)"
            ],
            "title": "Race Condition in Concurrent Redirect Following"
          },
          {
            "severity": "HIGH",
            "confidence": 78,
            "scenario": "What if redirect location contains malicious fragment (#) that breaks URL parsing and causes client to request unintended endpoints?",
            "impact": "Client makes requests to wrong URLs, potential SSRF if fragments manipulate host parsing, unexpected behavior.",
            "domains": [
              "universal (Input Validation)"
            ],
            "title": "Fragment Injection in Location Header"
          },
          {
            "severity": "MEDIUM",
            "confidence": 82,
            "scenario": "What if redirect from `api.example.com` to `evil.example.com` retains cookies due to loose domain matching?",
            "impact": "Session cookies leaked to untrusted subdomain, session hijacking if subdomain is compromised.",
            "domains": [
              "auth"
            ],
            "title": "Cookie Domain Confusion on Subdomain Redirects"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if `_is_https_redirect()` incorrectly identifies non-upgrade redirects as upgrades when custom ports are involved (http://site:8080 \u2192 https://site:8443)?",
            "impact": "Auth headers incorrectly preserved across what should be treated as cross-origin redirect, credential exposure.",
            "domains": [
              "auth"
            ],
            "title": "HTTP/HTTPS Upgrade Detection Edge Case"
          },
          {
            "severity": "MEDIUM",
            "confidence": 88,
            "scenario": "What if headers from intermediate redirects accumulate in the final request instead of being reset?",
            "impact": "Stale auth headers sent to wrong endpoints, cache headers causing incorrect caching behavior, debug headers leaking internal info.",
            "domains": [
              "auth",
              "api"
            ],
            "title": "Redirect Chain Header Accumulation"
          },
          {
            "severity": "MEDIUM",
            "confidence": 72,
            "scenario": "What if Location header contains Unicode characters that normalize differently causing origin comparison bypass?",
            "impact": "Cross-origin redirect treated as same-origin, auth headers leaked to attacker domain using Unicode spoofing.",
            "domains": [
              "universal (Input Validation)",
              "auth"
            ],
            "title": "Location Header Unicode Normalization Attack"
          },
          {
            "severity": "LOW",
            "confidence": 85,
            "scenario": "What if time-to-redirect varies significantly based on internal routing, leaking information about infrastructure?",
            "impact": "Internal network topology, endpoint existence, or routing logic exposed through timing analysis.",
            "domains": [
              "universal (Error Paths)"
            ],
            "title": "Redirect Timing Side Channel"
          }
        ],
        "matched_domains": [
          "auth",
          "image_processing",
          "api",
          "search"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 10,
          "critical": 2,
          "high": 3,
          "medium": 4,
          "low": 1
        }
      },
      "raw_response": "## \ud83d\udd34 CRITICAL (85%)\n\n**Auth Header Persistence Across Malicious Redirects**\n\n> What if an attacker controls a redirect destination and the auth header isn't properly stripped when crossing origins?\n\n- **Impact:** API keys, bearer tokens, or credentials leaked to attacker-controlled domains. Complete account compromise.\n- **Domain:** auth, api\n\n---\n\n## \ud83d\udd34 CRITICAL (90%)\n\n**Infinite Redirect Loop Resource Exhaustion**\n\n> What if circular redirect detection fails when URLs differ by query params, fragments, or encoding but resolve to same resource?\n\n- **Impact:** Memory exhaustion from growing redirect chain tracking, CPU spinning until timeout, DoS affecting all client requests.\n- **Domain:** universal (Resource Limits)\n\n---\n\n## \ud83d\udfe0 HIGH (80%)\n\n**Method Downgrade on Cross-Origin POST Redirect**\n\n> What if a POST to a payment endpoint redirects cross-origin and gets downgraded to GET, exposing sensitive data in URL logs?\n\n- **Impact:** Payment details, personal data exposed in server logs, referrer headers, browser history. GDPR/compliance violations.\n- **Domain:** auth, api\n\n---\n\n## \ud83d\udfe0 HIGH (75%)\n\n**Race Condition in Concurrent Redirect Following**\n\n> What if the same client follows redirects concurrently and redirect count tracking gets corrupted between threads?\n\n- **Impact:** max_redirects bypass leading to infinite loops, or premature termination of legitimate requests.\n- **Domain:** universal (Concurrency)\n\n---\n\n## \ud83d\udfe0 HIGH (78%)\n\n**Fragment Injection in Location Header**\n\n> What if redirect location contains malicious fragment (#) that breaks URL parsing and causes client to request unintended endpoints?\n\n- **Impact:** Client makes requests to wrong URLs, potential SSRF if fragments manipulate host parsing, unexpected behavior.\n- **Domain:** universal (Input Validation)\n\n---\n\n## \ud83d\udfe1 MEDIUM (82%)\n\n**Cookie Domain Confusion on Subdomain Redirects**\n\n> What if redirect from `api.example.com` to `evil.example.com` retains cookies due to loose domain matching?\n\n- **Impact:** Session cookies leaked to untrusted subdomain, session hijacking if subdomain is compromised.\n- **Domain:** auth\n\n---\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**HTTP/HTTPS Upgrade Detection Edge Case**\n\n> What if `_is_https_redirect()` incorrectly identifies non-upgrade redirects as upgrades when custom ports are involved (http://site:8080 \u2192 https://site:8443)?\n\n- **Impact:** Auth headers incorrectly preserved across what should be treated as cross-origin redirect, credential exposure.\n- **Domain:** auth\n\n---\n\n## \ud83d\udfe1 MEDIUM (88%)\n\n**Redirect Chain Header Accumulation**\n\n> What if headers from intermediate redirects accumulate in the final request instead of being reset?\n\n- **Impact:** Stale auth headers sent to wrong endpoints, cache headers causing incorrect caching behavior, debug headers leaking internal info.\n- **Domain:** auth, api\n\n---\n\n## \ud83d\udfe1 MEDIUM (72%)\n\n**Location Header Unicode Normalization Attack**\n\n> What if Location header contains Unicode characters that normalize differently causing origin comparison bypass?\n\n- **Impact:** Cross-origin redirect treated as same-origin, auth headers leaked to attacker domain using Unicode spoofing.\n- **Domain:** universal (Input Validation), auth\n\n---\n\n## \ud83d\udfe2 LOW (85%)\n\n**Redirect Timing Side Channel**\n\n> What if time-to-redirect varies significantly based on internal routing, leaking information about infrastructure?\n\n- **Impact:** Internal network topology, endpoint existence, or routing logic exposed through timing analysis.\n- **Domain:** universal (Error Paths)"
    },
    {
      "area": "SSL/TLS Configuration",
      "files": [
        "httpx/_config.py"
      ],
      "result": {
        "scope": "SSL/TLS and certificate handling in HTTP client - custom CA bundles, hostname verification, client certificates, environment variable trust_env, verify=False risk",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if attacker controls SSL_CERT_FILE or SSL_CERT_DIR environment variables?",
            "impact": "Complete TLS bypass - attacker can MITM all HTTPS traffic by providing malicious CA bundle that trusts their certificates. Silent data exfiltration without detection.",
            "domains": [
              "Configuration + External Dependencies"
            ],
            "title": "Environment Variable CA Bundle Poisoning"
          },
          {
            "severity": "CRITICAL",
            "confidence": 80,
            "scenario": "What if client certificate loading fails and stack traces log the certificate file paths or partial key data?",
            "impact": "Private keys exposed in application logs, enabling impersonation attacks and unauthorized access to certificate-protected services.",
            "domains": [
              "Error Paths + Configuration"
            ],
            "title": "Client Certificate Private Key Exposure in Logs"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if verify=False gets set in production due to development workarounds or configuration errors?",
            "impact": "All HTTPS connections become vulnerable to MITM attacks. Credentials, API keys, and sensitive data transmitted in clear to attackers.",
            "domains": [
              "Configuration + Deployment"
            ],
            "title": "verify=False Propagation in Production"
          },
          {
            "severity": "HIGH",
            "confidence": 85,
            "scenario": "What if SSL_CERT_FILE points to corrupted, empty, or deleted certificate file?",
            "impact": "SSL context creation fails silently or with cryptic errors, causing all HTTPS requests to fail. Service becomes unusable until fixed.",
            "domains": [
              "Infrastructure + External Dependencies"
            ],
            "title": "CA Bundle File Corruption or Deletion"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if developers pass pre-configured SSLContext that has check_hostname=False without realizing it?",
            "impact": "Connections succeed to wrong servers with valid certificates, enabling subtle MITM attacks that bypass certificate validation.",
            "domains": [
              "Configuration + Input Validation"
            ],
            "title": "Hostname Verification Bypass on Custom Contexts"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if certificate file is rotated/updated while load_cert_chain() is reading it?",
            "impact": "Partial certificate data loaded, causing SSL handshake failures and intermittent connection errors that are hard to diagnose.",
            "domains": [
              "Concurrency + Infrastructure"
            ],
            "title": "Client Certificate Chain Loading Race Condition"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if trust_env=True in production but SSL environment variables only exist in development?",
            "impact": "Different SSL behavior across environments - connections that work in dev fail in production, or worse, use different CA validation.",
            "domains": [
              "Deployment + Configuration"
            ],
            "title": "trust_env Inconsistency Between Environments"
          },
          {
            "severity": "MEDIUM",
            "confidence": 70,
            "scenario": "What if deprecated cert parameter is used repeatedly with different certificate files?",
            "impact": "SSL contexts accumulate in memory without cleanup, potentially causing memory growth and eventual OOM in long-running services.",
            "domains": [
              "Resource Limits + Configuration"
            ],
            "title": "Deprecated cert Parameter Memory Leak"
          },
          {
            "severity": "LOW",
            "confidence": 75,
            "scenario": "What if SSL_CERT_DIR contains symlinks that point outside expected certificate directories?",
            "impact": "SSL context loads unintended certificate files, potentially including private keys or certificates that shouldn't be trusted.",
            "domains": [
              "Configuration + Input Validation"
            ],
            "title": "Certificate Directory Traversal"
          }
        ],
        "matched_domains": [
          "deployment",
          "infrastructure"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 9,
          "critical": 2,
          "high": 3,
          "medium": 3,
          "low": 1
        }
      },
      "raw_response": "# \ud83d\udd0d SSL/TLS Risk Analysis\n\n## \ud83d\udd34 CRITICAL (85%)\n\n**Environment Variable CA Bundle Poisoning**\n\n> What if attacker controls SSL_CERT_FILE or SSL_CERT_DIR environment variables?\n\n- **Impact:** Complete TLS bypass - attacker can MITM all HTTPS traffic by providing malicious CA bundle that trusts their certificates. Silent data exfiltration without detection.\n- **Domain:** Configuration + External Dependencies\n\n## \ud83d\udd34 CRITICAL (80%)\n\n**Client Certificate Private Key Exposure in Logs**\n\n> What if client certificate loading fails and stack traces log the certificate file paths or partial key data?\n\n- **Impact:** Private keys exposed in application logs, enabling impersonation attacks and unauthorized access to certificate-protected services.\n- **Domain:** Error Paths + Configuration\n\n## \ud83d\udfe0 HIGH (90%)\n\n**verify=False Propagation in Production**\n\n> What if verify=False gets set in production due to development workarounds or configuration errors?\n\n- **Impact:** All HTTPS connections become vulnerable to MITM attacks. Credentials, API keys, and sensitive data transmitted in clear to attackers.\n- **Domain:** Configuration + Deployment\n\n## \ud83d\udfe0 HIGH (85%)\n\n**CA Bundle File Corruption or Deletion**\n\n> What if SSL_CERT_FILE points to corrupted, empty, or deleted certificate file?\n\n- **Impact:** SSL context creation fails silently or with cryptic errors, causing all HTTPS requests to fail. Service becomes unusable until fixed.\n- **Domain:** Infrastructure + External Dependencies\n\n## \ud83d\udfe0 HIGH (75%)\n\n**Hostname Verification Bypass on Custom Contexts**\n\n> What if developers pass pre-configured SSLContext that has check_hostname=False without realizing it?\n\n- **Impact:** Connections succeed to wrong servers with valid certificates, enabling subtle MITM attacks that bypass certificate validation.\n- **Domain:** Configuration + Input Validation\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**Client Certificate Chain Loading Race Condition**\n\n> What if certificate file is rotated/updated while load_cert_chain() is reading it?\n\n- **Impact:** Partial certificate data loaded, causing SSL handshake failures and intermittent connection errors that are hard to diagnose.\n- **Domain:** Concurrency + Infrastructure\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**trust_env Inconsistency Between Environments**\n\n> What if trust_env=True in production but SSL environment variables only exist in development?\n\n- **Impact:** Different SSL behavior across environments - connections that work in dev fail in production, or worse, use different CA validation.\n- **Domain:** Deployment + Configuration\n\n## \ud83d\udfe1 MEDIUM (70%)\n\n**Deprecated cert Parameter Memory Leak**\n\n> What if deprecated cert parameter is used repeatedly with different certificate files?\n\n- **Impact:** SSL contexts accumulate in memory without cleanup, potentially causing memory growth and eventual OOM in long-running services.\n- **Domain:** Resource Limits + Configuration\n\n## \ud83d\udfe2 LOW (75%)\n\n**Certificate Directory Traversal**\n\n> What if SSL_CERT_DIR contains symlinks that point outside expected certificate directories?\n\n- **Impact:** SSL context loads unintended certificate files, potentially including private keys or certificates that shouldn't be trusted.\n- **Domain:** Configuration + Input Validation"
    },
    {
      "area": "Multipart Uploads",
      "files": [
        "httpx/_multipart.py"
      ],
      "result": {
        "scope": "Multipart form data uploads - MIME type detection, file streaming, boundary generation, special character escaping, content-length calculation",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if a file claims small size in headers but streams gigabytes without length validation?",
            "impact": "OOM crash affecting all users, server becomes unresponsive",
            "domains": [
              "Resource Limits + File Upload"
            ],
            "title": "Memory Exhaustion via Large File Streaming"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if uploaded file content contains the exact multipart boundary sequence?",
            "impact": "Parser confusion, data corruption, potential data leakage between form fields",
            "domains": [
              "File Upload + Input Validation"
            ],
            "title": "Boundary Collision Attack"
          },
          {
            "severity": "HIGH",
            "confidence": 80,
            "scenario": "What if filename contains Unicode characters that get mangled during HTML5 form encoding?",
            "impact": "Files saved with wrong names, potential path traversal if encoding creates `../` sequences",
            "domains": [
              "Files + Input Validation"
            ],
            "title": "Unicode Filename Encoding Mismatch"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if multiple large unseekable streams (pipes, network streams) are processed simultaneously?",
            "impact": "Resource exhaustion, failed uploads for legitimate users",
            "domains": [
              "Resource Limits + Concurrency"
            ],
            "title": "File Handle Exhaustion via Unseekable Streams"
          },
          {
            "severity": "MEDIUM",
            "confidence": 85,
            "scenario": "What if field names contain UTF-8 characters that expand during URL encoding?",
            "impact": "HTTP protocol violations, request truncation, upstream proxy errors",
            "domains": [
              "Input Validation + External Dependencies"
            ],
            "title": "Content-Length Miscalculation with Mixed Encodings"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if filename is \"malware.txt.exe\" and `mimetypes.guess_type()` only sees the \".txt\"?",
            "impact": "Bypassed security filters, malicious files served with safe MIME types",
            "domains": [
              "File Upload + Input Validation"
            ],
            "title": "MIME Type Spoofing via Extension Manipulation"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if content length calculation fails due to unseekable file, but server doesn't support chunked encoding?",
            "impact": "Upload failures, inconsistent behavior across different server configurations",
            "domains": [
              "External Dependencies + Error Paths"
            ],
            "title": "Chunked Transfer Encoding Edge Case"
          },
          {
            "severity": "MEDIUM",
            "confidence": 70,
            "scenario": "What if the same file object is used in multiple concurrent multipart uploads?",
            "impact": "Corrupted uploads, partial data, one request affecting another",
            "domains": [
              "Concurrency + File Upload"
            ],
            "title": "Concurrent File Access Race Condition"
          }
        ],
        "matched_domains": [
          "file_upload",
          "api",
          "files"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 8,
          "critical": 1,
          "high": 3,
          "medium": 4,
          "low": 0
        }
      },
      "raw_response": "Looking at this multipart form data implementation, here are the risk scenarios I'm >70% confident could actually happen:\n\n## \ud83d\udd34 CRITICAL (85%)\n\n**Memory Exhaustion via Large File Streaming**\n\n> What if a file claims small size in headers but streams gigabytes without length validation?\n\n- **Impact:** OOM crash affecting all users, server becomes unresponsive\n- **Domain:** Resource Limits + File Upload\n- **Details:** `render_data()` reads `CHUNK_SIZE` (64KB) at a time with no total size limit. A crafted file that reports small size but streams indefinitely will consume all available memory.\n\n## \ud83d\udfe0 HIGH (90%)\n\n**Boundary Collision Attack**\n\n> What if uploaded file content contains the exact multipart boundary sequence?\n\n- **Impact:** Parser confusion, data corruption, potential data leakage between form fields\n- **Domain:** File Upload + Input Validation  \n- **Details:** `os.urandom(16).hex()` generates boundary, but file content isn't validated against it. If file contains `--{boundary}` sequence, it could terminate parsing early or inject fake form fields.\n\n## \ud83d\udfe0 HIGH (80%)\n\n**Unicode Filename Encoding Mismatch**\n\n> What if filename contains Unicode characters that get mangled during HTML5 form encoding?\n\n- **Impact:** Files saved with wrong names, potential path traversal if encoding creates `../` sequences\n- **Domain:** Files + Input Validation\n- **Details:** `_format_form_param()` only handles ASCII control chars and quotes. Unicode characters could be double-encoded or create unexpected byte sequences in filesystem paths.\n\n## \ud83d\udfe0 HIGH (75%)\n\n**File Handle Exhaustion via Unseekable Streams**\n\n> What if multiple large unseekable streams (pipes, network streams) are processed simultaneously?\n\n- **Impact:** Resource exhaustion, failed uploads for legitimate users\n- **Domain:** Resource Limits + Concurrency\n- **Details:** Code attempts `file.seek(0)` but catches `UnsupportedOperation` and continues. Unseekable streams stay open during entire multipart generation, potentially exhausting file descriptors under load.\n\n## \ud83d\udfe1 MEDIUM (85%)\n\n**Content-Length Miscalculation with Mixed Encodings**\n\n> What if field names contain UTF-8 characters that expand during URL encoding?\n\n- **Impact:** HTTP protocol violations, request truncation, upstream proxy errors\n- **Domain:** Input Validation + External Dependencies\n- **Details:** `get_content_length()` calculates length after rendering headers, but `_format_form_param()` percent-encodes some chars. Multi-byte UTF-8 in field names could cause length miscalculation.\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**MIME Type Spoofing via Extension Manipulation**\n\n> What if filename is \"malware.txt.exe\" and `mimetypes.guess_type()` only sees the \".txt\"?\n\n- **Impact:** Bypassed security filters, malicious files served with safe MIME types\n- **Domain:** File Upload + Input Validation\n- **Details:** `_guess_content_type()` relies on Python's `mimetypes` which uses last extension. Attackers can craft filenames to get desired MIME type while hiding actual file type.\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**Chunked Transfer Encoding Edge Case**\n\n> What if content length calculation fails due to unseekable file, but server doesn't support chunked encoding?\n\n- **Impact:** Upload failures, inconsistent behavior across different server configurations\n- **Domain:** External Dependencies + Error Paths\n- **Details:** `get_headers()` falls back to `Transfer-Encoding: chunked` when length unknown, but some proxies/servers reject chunked multipart uploads or have size limits that can't be enforced.\n\n## \ud83d\udfe1 MEDIUM (70%)\n\n**Concurrent File Access Race Condition**\n\n> What if the same file object is used in multiple concurrent multipart uploads?\n\n- **Impact:** Corrupted uploads, partial data, one request affecting another\n- **Domain:** Concurrency + File Upload\n- **Details:** `render_data()` calls `seek(0)` and then reads chunks. If same file object is shared across concurrent requests, seek position becomes unpredictable between threads."
    },
    {
      "area": "Request/Response Models",
      "files": [
        "httpx/_models.py",
        "httpx/_content.py"
      ],
      "result": {
        "scope": "HTTP request/response models and content handling - streaming, content encoding (gzip/deflate/brotli/zstd), charset detection, cookie jar, body consumption tracking",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if a malicious server sends a tiny gzipped response that decompresses to gigabytes?",
            "impact": "Memory exhaustion crashes the application, affecting all users. Attackers can DoS with minimal bandwidth.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Malicious Content-Encoding Bomb"
          },
          {
            "severity": "CRITICAL",
            "confidence": 80,
            "scenario": "What if malicious content uses encoding declaration mismatch (declares UTF-8 but contains Latin-1) to bypass input validation?",
            "impact": "Charset auto-detection could interpret malicious payloads differently than downstream parsers, enabling injection attacks or data corruption.",
            "domains": [
              "Input Validation"
            ],
            "title": "Charset Detection Bypass Attack"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if concurrent requests modify the same CookieJar instance simultaneously?",
            "impact": "Corrupted cookie state leads to authentication failures, session mixups between users, or lost session data requiring re-login.",
            "domains": [
              "Concurrency"
            ],
            "title": "Cookie Jar Race Condition"
          },
          {
            "severity": "HIGH",
            "confidence": 85,
            "scenario": "What if code calls `response.read()` twice on a consumed stream without checking `StreamConsumed` exception?",
            "impact": "Silent data loss or empty responses in retry scenarios. Business logic receives partial data thinking it's complete.",
            "domains": [
              "State & Data"
            ],
            "title": "Stream Double-Consumption"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if headers contain mixed encodings that cause the encoding detection loop to process massive headers repeatedly?",
            "impact": "CPU exhaustion during encoding detection with crafted headers containing strategic decode failures across ASCII\u2192UTF-8\u2192ISO-8859-1 chain.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Header Encoding Bomb via Fallback Chain"
          },
          {
            "severity": "MEDIUM",
            "confidence": 85,
            "scenario": "What if `Content-Type: text/html; charset=\"utf-8\\\"; malicious=script\"` bypasses the email.message parser?",
            "impact": "Malformed charset declarations could cause parsing inconsistencies between httpx and downstream systems, potentially enabling header injection.",
            "domains": [
              "Input Validation"
            ],
            "title": "Content-Type Charset Injection"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if a `MultiDecoder` chain fails mid-stream and leaves partial decoded data in buffers?",
            "impact": "Subsequent requests get corrupted data prepended from previous failed decode, causing parser errors or data integrity issues.",
            "domains": [
              "Error Paths"
            ],
            "title": "Decoder Chain State Corruption"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if `_parse_header_links()` processes a Link header with nested patterns like `<http://a.com/(((((((`?",
            "impact": "The `re.split(\", *<\", value)` with crafted input could trigger exponential backtracking, freezing request processing.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Link Header Regex DoS"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if network connection drops mid-stream but `UnattachedStream` isn't properly closed?",
            "impact": "File descriptors and memory leak over time as interrupted streams aren't cleaned up, eventually exhausting system resources.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Streaming Body Leak on Connection Drop"
          },
          {
            "severity": "LOW",
            "confidence": 75,
            "scenario": "What if upstream systems depend on exact header casing but the case-insensitive Headers dict changes it?",
            "impact": "Integration failures with legacy systems that expect `Content-Type` not `content-type`, causing API rejections or parsing errors.",
            "domains": [
              "External Dependencies"
            ],
            "title": "Headers Case Sensitivity Edge Case"
          }
        ],
        "matched_domains": [
          "api"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 10,
          "critical": 2,
          "high": 3,
          "medium": 4,
          "low": 1
        }
      },
      "raw_response": "## \ud83d\udd34 CRITICAL (85%)\n\n**Malicious Content-Encoding Bomb**\n\n> What if a malicious server sends a tiny gzipped response that decompresses to gigabytes?\n\n- **Impact:** Memory exhaustion crashes the application, affecting all users. Attackers can DoS with minimal bandwidth.\n- **Domain:** Resource Limits\n\n## \ud83d\udd34 CRITICAL (80%)\n\n**Charset Detection Bypass Attack**\n\n> What if malicious content uses encoding declaration mismatch (declares UTF-8 but contains Latin-1) to bypass input validation?\n\n- **Impact:** Charset auto-detection could interpret malicious payloads differently than downstream parsers, enabling injection attacks or data corruption.\n- **Domain:** Input Validation\n\n## \ud83d\udfe0 HIGH (90%)\n\n**Cookie Jar Race Condition**\n\n> What if concurrent requests modify the same CookieJar instance simultaneously?\n\n- **Impact:** Corrupted cookie state leads to authentication failures, session mixups between users, or lost session data requiring re-login.\n- **Domain:** Concurrency\n\n## \ud83d\udfe0 HIGH (85%)\n\n**Stream Double-Consumption**\n\n> What if code calls `response.read()` twice on a consumed stream without checking `StreamConsumed` exception?\n\n- **Impact:** Silent data loss or empty responses in retry scenarios. Business logic receives partial data thinking it's complete.\n- **Domain:** State & Data\n\n## \ud83d\udfe0 HIGH (75%)\n\n**Header Encoding Bomb via Fallback Chain**\n\n> What if headers contain mixed encodings that cause the encoding detection loop to process massive headers repeatedly?\n\n- **Impact:** CPU exhaustion during encoding detection with crafted headers containing strategic decode failures across ASCII\u2192UTF-8\u2192ISO-8859-1 chain.\n- **Domain:** Resource Limits\n\n## \ud83d\udfe1 MEDIUM (85%)\n\n**Content-Type Charset Injection**\n\n> What if `Content-Type: text/html; charset=\"utf-8\\\"; malicious=script\"` bypasses the email.message parser?\n\n- **Impact:** Malformed charset declarations could cause parsing inconsistencies between httpx and downstream systems, potentially enabling header injection.\n- **Domain:** Input Validation\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**Decoder Chain State Corruption**\n\n> What if a `MultiDecoder` chain fails mid-stream and leaves partial decoded data in buffers?\n\n- **Impact:** Subsequent requests get corrupted data prepended from previous failed decode, causing parser errors or data integrity issues.\n- **Domain:** Error Paths\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**Link Header Regex DoS**\n\n> What if `_parse_header_links()` processes a Link header with nested patterns like `<http://a.com/(((((((`?\n\n- **Impact:** The `re.split(\", *<\", value)` with crafted input could trigger exponential backtracking, freezing request processing.\n- **Domain:** Resource Limits\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**Streaming Body Leak on Connection Drop**\n\n> What if network connection drops mid-stream but `UnattachedStream` isn't properly closed?\n\n- **Impact:** File descriptors and memory leak over time as interrupted streams aren't cleaned up, eventually exhausting system resources.\n- **Domain:** Resource Limits\n\n## \ud83d\udfe2 LOW (75%)\n\n**Headers Case Sensitivity Edge Case**\n\n> What if upstream systems depend on exact header casing but the case-insensitive Headers dict changes it?\n\n- **Impact:** Integration failures with legacy systems that expect `Content-Type` not `content-type`, causing API rejections or parsing errors.\n- **Domain:** External Dependencies"
    },
    {
      "area": "URL Parsing",
      "files": [
        "httpx/_urls.py",
        "httpx/_urlparse.py"
      ],
      "result": {
        "scope": "URL parsing and query parameter handling - RFC-compliant parsing, IDNA internationalized domains, query encoding, username/password extraction, relative URL resolution",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if an attacker crafts a URL with mixed IDNA-encoded and Unicode characters that bypass host validation?",
            "impact": "Security controls that validate hostnames could be bypassed, enabling SSRF attacks or redirect to malicious domains. `url.host` shows decoded Unicode while `url.raw_host` shows encoded ASCII - inconsistent validation between these could allow domain confusion attacks.",
            "domains": [
              "security"
            ],
            "title": "IDNA Decoding Bypass via Mixed Unicode/ASCII"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if userinfo contains multiple colons or encoded colon characters that break username/password parsing?",
            "impact": "`username` and `password` properties use `partition(\":\")` which only splits on the first colon. URLs like `user%3Aname:pass%3Aword@host` could result in incorrect credential extraction, breaking auth flows or leaking sensitive data in logs.",
            "domains": [
              "auth"
            ],
            "title": "Username/Password Extraction from Malformed Userinfo"
          },
          {
            "severity": "HIGH",
            "confidence": 80,
            "scenario": "What if query parameters contain both raw bytes and URL-encoded strings that get double-encoded or inconsistently decoded?",
            "impact": "The `params` property creates `QueryParams` from raw query string, but kwargs handling converts bytes to ASCII. This encoding mismatch could cause parameter pollution attacks where `?user=admin&user%3D=hacker` creates ambiguous parameter sets.",
            "domains": [
              "universal (Input Validation)"
            ],
            "title": "Query Parameter Pollution via Encoding Mismatch"
          },
          {
            "severity": "HIGH",
            "confidence": 75,
            "scenario": "What if non-standard schemes use default ports that aren't in the normalization list?",
            "impact": "Port normalization only handles \"http\", \"https\", \"ws\", \"wss\", and \"ftp\". Custom schemes like `redis://host:6379` won't normalize, causing `URL(\"redis://host\") != URL(\"redis://host:6379\")` even if 6379 is Redis's default port. This breaks URL equality checks and caching.",
            "domains": [
              "universal (State & Data)"
            ],
            "title": "Port Normalization Logic Confusion"
          },
          {
            "severity": "MEDIUM",
            "confidence": 85,
            "scenario": "What if `host` property encounters IDNA decoding failure on malformed xn-- domains?",
            "impact": "`idna.decode(host)` can throw exceptions for malformed punycode. The code has no try/catch, so malformed domains like `xn--invalid` would crash the property access rather than gracefully falling back to raw representation.",
            "domains": [
              "universal (Error Paths)"
            ],
            "title": "IDNA Exception Handling on Malformed Unicode"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if `raw_path` and `path` properties get out of sync due to encoding edge cases?",
            "impact": "`raw_path` is used for HTTP requests while `path` is for display/logic. If certain byte sequences decode differently than they encode (normalization differences), the actual request path might differ from what application logic expects, causing routing failures.",
            "domains": [
              "universal (State & Data)"
            ],
            "title": "Raw Path vs Path Encoding Inconsistency"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if fragment contains characters that break when reconstructing URLs?",
            "impact": "Fragment property is mentioned in docstring but not visible in the provided code. If fragments aren't properly encoded/decoded consistently with other URL components, URL reconstruction could generate invalid URLs or lose fragment data.",
            "domains": [
              "universal (Input Validation)"
            ],
            "title": "Fragment Handling in URL Construction"
          },
          {
            "severity": "MEDIUM",
            "confidence": 70,
            "scenario": "What if kwargs validation accepts None values but downstream code assumes non-None?",
            "impact": "The type checking explicitly allows None values (`if value is not None`), but properties like `netloc.encode(\"ascii\")` would fail if the underlying `_uri_reference.netloc` is None. This creates a validation gap.",
            "domains": [
              "universal (Input Validation)"
            ],
            "title": "Kwargs Type Validation Bypass"
          }
        ],
        "matched_domains": [
          "auth",
          "database",
          "security",
          "search"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 8,
          "critical": 1,
          "high": 3,
          "medium": 4,
          "low": 0
        }
      },
      "raw_response": "## \ud83d\udd34 CRITICAL (85%)\n\n**IDNA Decoding Bypass via Mixed Unicode/ASCII**\n\n> What if an attacker crafts a URL with mixed IDNA-encoded and Unicode characters that bypass host validation?\n\n- **Impact:** Security controls that validate hostnames could be bypassed, enabling SSRF attacks or redirect to malicious domains. `url.host` shows decoded Unicode while `url.raw_host` shows encoded ASCII - inconsistent validation between these could allow domain confusion attacks.\n- **Domain:** security\n\n## \ud83d\udfe0 HIGH (90%)\n\n**Username/Password Extraction from Malformed Userinfo**\n\n> What if userinfo contains multiple colons or encoded colon characters that break username/password parsing?\n\n- **Impact:** `username` and `password` properties use `partition(\":\")` which only splits on the first colon. URLs like `user%3Aname:pass%3Aword@host` could result in incorrect credential extraction, breaking auth flows or leaking sensitive data in logs.\n- **Domain:** auth\n\n## \ud83d\udfe0 HIGH (80%)\n\n**Query Parameter Pollution via Encoding Mismatch**\n\n> What if query parameters contain both raw bytes and URL-encoded strings that get double-encoded or inconsistently decoded?\n\n- **Impact:** The `params` property creates `QueryParams` from raw query string, but kwargs handling converts bytes to ASCII. This encoding mismatch could cause parameter pollution attacks where `?user=admin&user%3D=hacker` creates ambiguous parameter sets.\n- **Domain:** universal (Input Validation)\n\n## \ud83d\udfe0 HIGH (75%)\n\n**Port Normalization Logic Confusion**\n\n> What if non-standard schemes use default ports that aren't in the normalization list?\n\n- **Impact:** Port normalization only handles \"http\", \"https\", \"ws\", \"wss\", and \"ftp\". Custom schemes like `redis://host:6379` won't normalize, causing `URL(\"redis://host\") != URL(\"redis://host:6379\")` even if 6379 is Redis's default port. This breaks URL equality checks and caching.\n- **Domain:** universal (State & Data)\n\n## \ud83d\udfe1 MEDIUM (85%)\n\n**IDNA Exception Handling on Malformed Unicode**\n\n> What if `host` property encounters IDNA decoding failure on malformed xn-- domains?\n\n- **Impact:** `idna.decode(host)` can throw exceptions for malformed punycode. The code has no try/catch, so malformed domains like `xn--invalid` would crash the property access rather than gracefully falling back to raw representation.\n- **Domain:** universal (Error Paths)\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**Raw Path vs Path Encoding Inconsistency**\n\n> What if `raw_path` and `path` properties get out of sync due to encoding edge cases?\n\n- **Impact:** `raw_path` is used for HTTP requests while `path` is for display/logic. If certain byte sequences decode differently than they encode (normalization differences), the actual request path might differ from what application logic expects, causing routing failures.\n- **Domain:** universal (State & Data)\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**Fragment Handling in URL Construction**\n\n> What if fragment contains characters that break when reconstructing URLs?\n\n- **Impact:** Fragment property is mentioned in docstring but not visible in the provided code. If fragments aren't properly encoded/decoded consistently with other URL components, URL reconstruction could generate invalid URLs or lose fragment data.\n- **Domain:** universal (Input Validation)\n\n## \ud83d\udfe1 MEDIUM (70%)\n\n**Kwargs Type Validation Bypass**\n\n> What if kwargs validation accepts None values but downstream code assumes non-None?\n\n- **Impact:** The type checking explicitly allows None values (`if value is not None`), but properties like `netloc.encode(\"ascii\")` would fail if the underlying `_uri_reference.netloc` is None. This creates a validation gap.\n- **Domain:** universal (Input Validation)"
    },
    {
      "area": "Content Decoders & Error Handling",
      "files": [
        "httpx/_decoders.py",
        "httpx/_exceptions.py"
      ],
      "result": {
        "scope": "HTTP content decoders and error handling - gzip/deflate/brotli/zstandard decoders, stream error recovery, timeout hierarchies, connection error mapping",
        "risks": [
          {
            "severity": "CRITICAL",
            "confidence": 85,
            "scenario": "What if an attacker sends a small compressed payload that expands to gigabytes when decompressed?",
            "impact": "OOM crash bringing down the entire service. Classic zip bomb attack vector. All decoders (gzip, brotli, zstd) decompress without size limits.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Memory bomb through malicious compression"
          },
          {
            "severity": "CRITICAL",
            "confidence": 80,
            "scenario": "What if malicious zstd data creates a cycle where `unused_data` keeps producing more frames that reference each other?",
            "impact": "CPU spin loop, service hangs indefinitely. The `while` loop in ZStandardDecoder assumes forward progress but doesn't validate frame integrity.",
            "domains": [
              "External Dependencies / Input Validation"
            ],
            "title": "Infinite loop in ZStandard multi-frame handling"
          },
          {
            "severity": "HIGH",
            "confidence": 90,
            "scenario": "What if a client opens thousands of concurrent streaming connections and never calls flush(), leaving decompressor objects with internal buffers hanging?",
            "impact": "Memory leak as each decoder maintains internal state. No cleanup mechanism for abandoned decoders. Gradual service degradation.",
            "domains": [
              "Resource Limits"
            ],
            "title": "Resource exhaustion through decoder state accumulation"
          },
          {
            "severity": "HIGH",
            "confidence": 85,
            "scenario": "What if deployment switches from `brotli` to `brotlicffi` package and the method detection logic (`hasattr` checks) fails or behaves differently under load?",
            "impact": "Runtime AttributeError crashes for all brotli-compressed requests. The dual-package support is fragile and untested in production.",
            "domains": [
              "Configuration"
            ],
            "title": "Brotli package switching breaks production"
          },
          {
            "severity": "HIGH",
            "confidence": 80,
            "scenario": "What if one decoder in the chain fails after partially processing data, but earlier decoders already consumed and transformed the input?",
            "impact": "Data corruption or truncation. The chain processes sequentially but has no rollback mechanism. Partial success creates inconsistent state.",
            "domains": [
              "Error Paths"
            ],
            "title": "MultiDecoder amplifies partial failures"
          },
          {
            "severity": "MEDIUM",
            "confidence": 85,
            "scenario": "What if chunk_size is large but incoming data arrives in tiny increments, causing the internal BytesIO buffer to grow but never flush?",
            "impact": "Memory accumulation per connection. The buffer only flushes when reaching chunk_size, but small increments below threshold accumulate indefinitely.",
            "domains": [
              "Resource Limits"
            ],
            "title": "ByteChunker buffer grows unbounded"
          },
          {
            "severity": "MEDIUM",
            "confidence": 80,
            "scenario": "What if the first deflate attempt fails but consumes/modifies the input data before falling back to the second decompressor instance?",
            "impact": "Data corruption or failed decompression. The fallback mechanism doesn't preserve original input state between attempts.",
            "domains": [
              "Error Paths"
            ],
            "title": "Deflate fallback creates double-processing vulnerability"
          },
          {
            "severity": "MEDIUM",
            "confidence": 75,
            "scenario": "What if Content-Type charset changes mid-stream or conflicts with actual byte encoding, but TextDecoder was initialized with hardcoded 'utf-8'?",
            "impact": "Mojibake or replacement characters in decoded text. The encoding is set at initialization and can't adapt to runtime content discovery.",
            "domains": [
              "Input Validation"
            ],
            "title": "TextDecoder encoding assumption breaks on dynamic headers"
          },
          {
            "severity": "MEDIUM",
            "confidence": 70,
            "scenario": "What if flush() is called on MultiDecoder but some child decoders have internal state that requires specific flush ordering to complete properly?",
            "impact": "Truncated or incomplete decoded data. The flush implementation processes children in reverse order but doesn't handle interdependencies.",
            "domains": [
              "State & Data"
            ],
            "title": "Flush ordering in MultiDecoder creates incomplete output"
          }
        ],
        "matched_domains": [
          "database"
        ],
        "pattern_count": 7,
        "depth": "deep",
        "threshold": 70,
        "summary": {
          "total_risks": 9,
          "critical": 2,
          "high": 3,
          "medium": 4,
          "low": 0
        }
      },
      "raw_response": "# Gremlin Risk Analysis: HTTP Content Decoders\n\n## \ud83d\udd34 CRITICAL (85%)\n\n**Memory bomb through malicious compression**\n\n> What if an attacker sends a small compressed payload that expands to gigabytes when decompressed?\n\n- **Impact:** OOM crash bringing down the entire service. Classic zip bomb attack vector. All decoders (gzip, brotli, zstd) decompress without size limits.\n- **Domain:** Resource Limits\n\n---\n\n## \ud83d\udd34 CRITICAL (80%)\n\n**Infinite loop in ZStandard multi-frame handling**\n\n> What if malicious zstd data creates a cycle where `unused_data` keeps producing more frames that reference each other?\n\n- **Impact:** CPU spin loop, service hangs indefinitely. The `while` loop in ZStandardDecoder assumes forward progress but doesn't validate frame integrity.\n- **Domain:** External Dependencies / Input Validation\n\n---\n\n## \ud83d\udfe0 HIGH (90%)\n\n**Resource exhaustion through decoder state accumulation**\n\n> What if a client opens thousands of concurrent streaming connections and never calls flush(), leaving decompressor objects with internal buffers hanging?\n\n- **Impact:** Memory leak as each decoder maintains internal state. No cleanup mechanism for abandoned decoders. Gradual service degradation.\n- **Domain:** Resource Limits\n\n---\n\n## \ud83d\udfe0 HIGH (85%)\n\n**Brotli package switching breaks production**\n\n> What if deployment switches from `brotli` to `brotlicffi` package and the method detection logic (`hasattr` checks) fails or behaves differently under load?\n\n- **Impact:** Runtime AttributeError crashes for all brotli-compressed requests. The dual-package support is fragile and untested in production.\n- **Domain:** Configuration\n\n---\n\n## \ud83d\udfe0 HIGH (80%)\n\n**MultiDecoder amplifies partial failures**\n\n> What if one decoder in the chain fails after partially processing data, but earlier decoders already consumed and transformed the input?\n\n- **Impact:** Data corruption or truncation. The chain processes sequentially but has no rollback mechanism. Partial success creates inconsistent state.\n- **Domain:** Error Paths\n\n---\n\n## \ud83d\udfe1 MEDIUM (85%)\n\n**ByteChunker buffer grows unbounded**\n\n> What if chunk_size is large but incoming data arrives in tiny increments, causing the internal BytesIO buffer to grow but never flush?\n\n- **Impact:** Memory accumulation per connection. The buffer only flushes when reaching chunk_size, but small increments below threshold accumulate indefinitely.\n- **Domain:** Resource Limits\n\n---\n\n## \ud83d\udfe1 MEDIUM (80%)\n\n**Deflate fallback creates double-processing vulnerability**\n\n> What if the first deflate attempt fails but consumes/modifies the input data before falling back to the second decompressor instance?\n\n- **Impact:** Data corruption or failed decompression. The fallback mechanism doesn't preserve original input state between attempts.\n- **Domain:** Error Paths\n\n---\n\n## \ud83d\udfe1 MEDIUM (75%)\n\n**TextDecoder encoding assumption breaks on dynamic headers**\n\n> What if Content-Type charset changes mid-stream or conflicts with actual byte encoding, but TextDecoder was initialized with hardcoded 'utf-8'?\n\n- **Impact:** Mojibake or replacement characters in decoded text. The encoding is set at initialization and can't adapt to runtime content discovery.\n- **Domain:** Input Validation\n\n---\n\n## \ud83d\udfe1 MEDIUM (70%)\n\n**Flush ordering in MultiDecoder creates incomplete output**\n\n> What if flush() is called on MultiDecoder but some child decoders have internal state that requires specific flush ordering to complete properly?\n\n- **Impact:** Truncated or incomplete decoded data. The flush implementation processes children in reverse order but doesn't handle interdependencies.\n- **Domain:** State & Data"
    }
  ]
}